{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.options.display.max_columns=999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/home-data-for-ml-course/train.csv')\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"x가 커지면 커질수록 y값도 커지는지 체크, 숫자적 칼럼있는지"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['SalePrice'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n#모든 칼럼에 레이블링 한 번에 하는 법\n#문자형 칼럼에 접근하는 방법\n#결측치는 숫자형으로 받아들여진다. \ntrain.columns #모든 칼럼 출력할 수 있다. \nc=train.columns[train.dtypes==object] #조건 부여 옵션 \nfor i in c:\n    train[i]=le.fit_transform(list(train[i])) #먼저 리스트처리 해주기, list만나면 형식이 카테고리화 된다. 우리 모델이 학습을 할 수 있음. 결측치도 다 의미있음! \n    #eg.보험데이터의 경우, 공란으로 둔 설문이 있으면, 데이터 수집 시 결측치로 나오겠지? 근데 이것도 정보로 체크해서 받아들임, 이것도 학습해야해,그래서 카테고리화 시켜준다. \n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('../input/home-data-for-ml-course/test.csv')\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=test.columns[test.dtypes==object] #조건 부여 옵션 \nfor i in c:\n    test[i]=le.fit_transform(list(test[i])) #먼저 리스트처리 해주기, list만나면 형식이 카테고리화 된다. 우리 모델이 학습을 할 수 있음. 결측치도 다 의미있음! \n    #eg.보험데이터의 경우, 공란으로 둔 설문이 있으면, 데이터 수집 시 결측치로 나오겠지? 근데 이것도 정보로 체크해서 받아들임, 이것도 학습해야해,그래서 카테고리화 시켜준다. \n    print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=pd.read_csv('../input/home-data-for-ml-course/sample_submission.csv')\nk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2=train.drop(columns=['Id','SalePrice'])\ntest_2=test.drop(columns=['Id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2=train_2.fillna(0)\ntest_2=test_2.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_2.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.칼럼이 카테고리형이라 ->  tree 모델이 점수가 더 잘 나왔었음 \n2. 전 처리시 숫자형으로 바꿨을 때 \n3. 데이터 패턴을 더욱 체크해보기 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.ensemble import RandomForestRegressor\n#rf = RandomForestRegressor(n_jobs=4)\n#rf.fit(train_2, train['SalePrice']) \n#result = rf.predict(test_2)\n#result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"xgboost -> 나무 생성할 때 마다 학습이 자꾸 발전됨 -> 벗 과대적합의 위험성이 있음 \n옵션 하이퍼 파라미터 잘 설정해라 \n1. learning rate 잘 설정해야함 (기본 값이 이상해서 0.3/ 너무 크다)\n학습률: 모델이 얼마나 빠르게 학습할까를 정하는 것이다. 0.3은 놓지는 부분이 많음/ 세세하게 학습 ㄴㄴ 하게 됨\n2. 결국 모델은 W 값을 잘 찾는게 중요하다. 어떤 input에도 / 지나쳐서 global optimal 을 놓치게 되니까유 -> 시간은 더 걸려도 최적의 값 찾을 수 있으니까\n3. 0.1로만 하는 이유 너무 줄이면 학습 시 너무 모멘텀이 작아서 멈춰버림, local optimal에 안주해버림 \n4. 칼럼들의 속성들이 거리기반인지(선형모델) / 카테고리식인지/supportvector machine/ knn  등 모델을 다양하게 사용해야한다. \neg. 트리 모델은 구분 위주, 의사결정위주니까 숫자적인지 아닌지 체크"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor(learning_rate=0.1) #이런 옵션들에 대해서 3~4개로 체크 \nxgb.fit(train_2,train['SalePrice'])\nresult=xgb.predict(test_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.read_csv('../input/home-data-for-ml-course/sample_submission.csv')\nsub['SalePrice']=result\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('house7.csv',index=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}